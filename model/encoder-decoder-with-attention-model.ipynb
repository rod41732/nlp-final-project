{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NLP_Preprocess_Final_Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmxm6BX8DuUE",
        "outputId": "ba43209e-ba46-449b-e8e3-4b084ccd4366"
      },
      "source": [
        "import tensorflow\n",
        "\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm9YcvEiG0ph"
      },
      "source": [
        "# Ref & Quicklink: \n",
        "\n",
        "- [Text Summarize](https://towardsdatascience.com/text-summarization-from-scratch-using-encoder-decoder-network-with-attention-in-keras-5fa80d12710e)\n",
        "- [thaisum](https://huggingface.co/datasets/thaisum)\n",
        "- [code Medium](https://gist.github.com/VarunSaravanan)\n",
        "- [Attention](https://github.com/thushv89/attention_keras)\n",
        "- [colab](https://colab.research.google.com/drive/1euy7-fIJwTjoP26adlkr5o_da9_GFqFp)\n",
        "- [Google Drive](https://drive.google.com/open?id=15dfGrjIZ9W4jfHgku0cUl_mmqqdt_xde&authuser=6031020321%40student.chula.ac.th&usp=drive_fs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJy_4QgngI2v"
      },
      "source": [
        "load_pre = True\n",
        "load_weight = True\n",
        "plot_image = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvgjzzK5Zbiz",
        "outputId": "a35c74c0-fc9f-483b-edcc-f714e7303b1a"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun May  2 03:37:05 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5dcVWhEF4wb"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IoS92ZDvnK0",
        "outputId": "9bc1ef5d-691c-4c47-baf2-db8f97b7258d"
      },
      "source": [
        "!pip install pythainlp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pythainlp in /usr/local/lib/python3.7/dist-packages (2.3.1)\n",
            "Requirement already satisfied: python-crfsuite>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from pythainlp) (0.9.7)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from pythainlp) (2.23.0)\n",
            "Requirement already satisfied: tinydb>=3.0 in /usr/local/lib/python3.7/dist-packages (from pythainlp) (4.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-PNIq7xG8uG"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import warnings\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JXyZn3HGkkQ"
      },
      "source": [
        "## Download data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNVYLZemVd5D",
        "outputId": "320c4608-13a3-41e8-a867-fb30798c0379"
      },
      "source": [
        "if load_pre :\n",
        "  # load from prepocess\n",
        "  #Train data\n",
        "  !gdown --id 1GfPMhYq9kXGwOMqx_tJzIIxcg061-pMQ\n",
        "  #Test data\n",
        "  !gdown --id 15e8MiMNNhgm16v4qlm19dx9bubS7gAmw\n",
        "  #Validation data\n",
        "  !gdown --id 15f9CcEICmw3o56cOac14N_mZ1pWMCiwG\n",
        "else :\n",
        "  #Train data\n",
        "  !gdown --id 1jhXHwN6oYnGnWlzyKl12PTO3WSCFGJX6\n",
        "  #Test data\n",
        "  !gdown --id 1-08jI8lJZdQQa8XBQKjahx2jwygUFGto\n",
        "  #Validation data\n",
        "  !gdown --id 1-0KeeB8J770e5-DazCwRPbpNjgKlPmG3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1GfPMhYq9kXGwOMqx_tJzIIxcg061-pMQ\n",
            "To: /content/train_token.csv\n",
            "18.7MB [00:00, 87.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=15e8MiMNNhgm16v4qlm19dx9bubS7gAmw\n",
            "To: /content/test_token.csv\n",
            "10.2MB [00:00, 89.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=15f9CcEICmw3o56cOac14N_mZ1pWMCiwG\n",
            "To: /content/val_token.csv\n",
            "10.2MB [00:00, 89.8MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9SQUSxh_wAP"
      },
      "source": [
        "SAVE_DIRECTORY = '/content'\n",
        "\n",
        "if load_pre :\n",
        "  df_train = pd.read_csv(f'{SAVE_DIRECTORY}/train_token.csv',encoding='utf-8-sig', nrows=20000) # read only first 20k rows\n",
        "  df_test = pd.read_csv(f'{SAVE_DIRECTORY}/test_token.csv',encoding='utf-8-sig')\n",
        "  df_validation = pd.read_csv(f'{SAVE_DIRECTORY}/val_token.csv',encoding='utf-8-sig')\n",
        "else :\n",
        "  df_train = pd.read_csv(f'{SAVE_DIRECTORY}/thaisum_train.csv',encoding='utf-8-sig', nrows=20000)\n",
        "  df_test = pd.read_csv(f'{SAVE_DIRECTORY}/thaisum_test.csv',encoding='utf-8-sig')\n",
        "  df_validation = pd.read_csv(f'{SAVE_DIRECTORY}/thaisum_validation.csv',encoding='utf-8-sig')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHdg1hwRGw2K"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maV2cJydEEiZ"
      },
      "source": [
        "Now we need to clean our text, we perform the following steps for the text \n",
        "and headlines pair:\n",
        "\n",
        "- Remove extra white spaces\n",
        "- Expand contractions\n",
        "- Remove special characters\n",
        "- Lowercase all texts\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SXYKL7RBUKe"
      },
      "source": [
        "# Select only summary and title\n",
        "if not load_pre:\n",
        "  df_train_clean = df_train[[\"summary\", \"title\"]]\n",
        "  df_test_clean = df_test[[\"summary\", \"title\"]]\n",
        "  df_validation_clean = df_validation[[\"summary\", \"title\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4euC0QlLCazf"
      },
      "source": [
        "# Remove special characters\n",
        "if not load_pre:\n",
        "  from unicodedata import normalize\n",
        "\n",
        "  # n space -> 1 space\n",
        "  # lowercase \n",
        "  def cleanInput(sentence):\n",
        "    sentence = normalize(\"NFKD\", sentence.strip().lower())\n",
        "    sentence = \" \".join(sentence.split())\n",
        "    return sentence\n",
        "\n",
        "  df_train_clean[\"summary\"] = df_train_clean[\"summary\"].apply(cleanInput)\n",
        "  df_train_clean[\"title\"] = df_train_clean[\"title\"].apply(cleanInput)\n",
        "  df_test_clean[\"summary\"] = df_test_clean[\"summary\"].apply(cleanInput)\n",
        "  df_test_clean[\"title\"] = df_test_clean[\"title\"].apply(cleanInput)\n",
        "  df_validation_clean[\"summary\"] = df_validation_clean[\"summary\"].apply(cleanInput)\n",
        "  df_validation_clean[\"title\"] = df_validation_clean[\"title\"].apply(cleanInput)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dpz7ujEEWbi"
      },
      "source": [
        "- add start and end tokens \n",
        "-  help us to get an overall idea about the distribution of length of the text. This will help us fix the maximum length of the sequence\n",
        "- Train test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTZQqgf6efxG"
      },
      "source": [
        "# tokenize summary and title\n",
        "if not load_pre:\n",
        "  from pythainlp.tokenize import word_tokenize\n",
        "\n",
        "  # split number\n",
        "  def isNum(word):\n",
        "    return word.replace(\",\", \"\").replace(\".\", \"\").isnumeric()\n",
        "\n",
        "  def clearAfterToken(sentence):\n",
        "    newSentence = []\n",
        "    for word in sentence:\n",
        "      word = word.strip()\n",
        "      if isNum(word):\n",
        "        word = \"~\".join(list(word))\n",
        "      \n",
        "      word = word.replace(\"(\", \"~(~\").replace(\")\", \"~)~\").replace(\"–\", \"-\"). replace(\"-\", \"~-~\").replace(\"?\", \"~?~\")\n",
        "      word = word.replace(\"“\",'~\"~').replace(\"”\",'~\"~')\n",
        "      word = word.replace(\"‘\",\"~'~\").replace(\"’\",\"~'~\")\n",
        "      word = word.strip().split(\"~\")\n",
        "      newSentence += word\n",
        "    return newSentence\n",
        "\n",
        "\n",
        "  for i in tqdm(range(df_train.shape[0])):\n",
        "    df_train_clean[\"summary\"][i] = clearAfterToken(word_tokenize(df_train_clean[\"summary\"][i], engine=\"newmm\"))\n",
        "    df_train_clean[\"title\"][i] = clearAfterToken(word_tokenize(df_train_clean[\"title\"][i], engine=\"newmm\"))\n",
        "\n",
        "  for i in tqdm(range(df_test.shape[0])):\n",
        "    df_test_clean[\"summary\"][i] = clearAfterToken(word_tokenize(df_test_clean[\"summary\"][i], engine=\"newmm\"))\n",
        "    df_test_clean[\"title\"][i] = clearAfterToken(word_tokenize(df_test_clean[\"title\"][i], engine=\"newmm\"))\n",
        "\n",
        "  for i in tqdm(range(df_validation.shape[0])):\n",
        "    df_validation_clean[\"summary\"][i] = clearAfterToken(word_tokenize(df_validation_clean[\"summary\"][i], engine=\"newmm\"))\n",
        "    df_validation_clean[\"title\"][i] = clearAfterToken(word_tokenize(df_validation_clean[\"title\"][i], engine=\"newmm\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eiUBzFIEQ_q"
      },
      "source": [
        "if not load_pre:\n",
        "  df_train_clean.to_csv(\"train_token.csv\", index=False)\n",
        "  df_test_clean.to_csv(\"test_token.csv\", index=False)\n",
        "  df_validation_clean.to_csv(\"val_token.csv\", index=False)\n",
        "\n",
        "  from google.colab import files\n",
        "  files.download(\"train_token.csv\") \n",
        "  files.download(\"test_token.csv\") \n",
        "  files.download(\"val_token.csv\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7CHz0-eg2p3"
      },
      "source": [
        "# handle pre-preprocess data \n",
        "if load_pre:\n",
        "  df_train_clean = df_train.copy()\n",
        "  df_test_clean = df_test.copy()\n",
        "  df_validation_clean = df_validation.copy()\n",
        "\n",
        "  df_train_clean[\"summary\"] = df_train[\"summary\"].apply(lambda x: x[2:-2].split(\"', '\"))\n",
        "  df_train_clean[\"title\"] = df_train[\"title\"].apply(lambda x: x[2:-2].split(\"', '\"))\n",
        "  df_test_clean[\"summary\"] = df_test[\"summary\"].apply(lambda x: x[2:-2].split(\"', '\"))\n",
        "  df_test_clean[\"title\"] = df_test[\"title\"].apply(lambda x: x[2:-2].split(\"', '\"))\n",
        "  df_validation_clean[\"summary\"] = df_validation[\"summary\"].apply(lambda x: x[2:-2].split(\"', '\"))\n",
        "  df_validation_clean[\"title\"] = df_validation[\"title\"].apply(lambda x: x[2:-2].split(\"', '\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYoPUK2ZGCna"
      },
      "source": [
        "# add start and end token\n",
        "START_TOKEN = \"<s>\"\n",
        "END_TOKEN = \"</s>\"\n",
        "UNK_TOKEN = \"UNK\"\n",
        "\n",
        "df_train_clean[\"title\"] = df_train_clean[\"title\"].apply(lambda x: [START_TOKEN] + x + [END_TOKEN])\n",
        "df_test_clean[\"title\"] = df_test_clean[\"title\"].apply(lambda x: [START_TOKEN] + x + [END_TOKEN])\n",
        "df_validation_clean[\"title\"] = df_validation_clean[\"title\"].apply(lambda x: [START_TOKEN] + x + [END_TOKEN])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfHFQ-EteofE"
      },
      "source": [
        "def wordMap(sentence_list):\n",
        "  word2idx = {}\n",
        "  word2idx[UNK_TOKEN] = 1\n",
        "  word2idx[START_TOKEN] = 2\n",
        "  word2idx[END_TOKEN] = 3\n",
        "\n",
        "  for sentence in sentence_list:\n",
        "    for word in sentence:\n",
        "      if word not in word2idx:\n",
        "        word2idx[word] = len(word2idx) + 1\n",
        "  \n",
        "  idx2word = {v: k for k, v in word2idx.items()}\n",
        "  return word2idx, idx2word\n",
        "\n",
        "word2idx, idx2word = wordMap(df_train_clean[\"summary\"].tolist() + df_train_clean[\"title\"].tolist())\n",
        "\n",
        "df_train_clean[\"summary\"] = df_train_clean[\"summary\"].apply(lambda x: [word2idx.get(word, 1) for word in x])\n",
        "df_train_clean[\"title\"] = df_train_clean[\"title\"].apply(lambda x: [word2idx.get(word, 1) for word in x])\n",
        "df_test_clean[\"summary\"] = df_test_clean[\"summary\"].apply(lambda x: [word2idx.get(word, 1) for word in x])\n",
        "df_test_clean[\"title\"] = df_test_clean[\"title\"].apply(lambda x: [word2idx.get(word, 1) for word in x])\n",
        "df_validation_clean[\"summary\"] = df_validation_clean[\"summary\"].apply(lambda x: [word2idx.get(word, 1) for word in x])\n",
        "df_validation_clean[\"title\"] = df_validation_clean[\"title\"].apply(lambda x: [word2idx.get(word, 1) for word in x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6LAnPptejBy",
        "outputId": "64b1a345-024a-45e8-fbe6-03807a644cb4"
      },
      "source": [
        "maxlen_summary = max([len(x) for x in df_train_clean[\"summary\"]])\n",
        "maxlen_title = max([len(x) for x in df_train_clean[\"title\"]])\n",
        "print(\"maxlen_summary: \", maxlen_summary)\n",
        "print(\"maxlen_title: \", maxlen_title)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "maxlen_summary:  439\n",
            "maxlen_title:  48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDlrLUjgd7xo"
      },
      "source": [
        "# padding\n",
        "X_train = pad_sequences(df_train_clean[\"summary\"], maxlen=maxlen_summary, dtype='int32', padding='post', truncating='post',value=0)\n",
        "y_train = pad_sequences(df_train_clean[\"title\"], maxlen=maxlen_title, dtype='int32', padding='post', truncating='post',value=0)\n",
        "X_test = pad_sequences(df_test_clean[\"summary\"], maxlen=maxlen_summary, dtype='int32', padding='post', truncating='post',value=0)\n",
        "y_test = pad_sequences(df_test_clean[\"title\"], maxlen=maxlen_title, dtype='int32', padding='post', truncating='post',value=0)\n",
        "X_val = pad_sequences(df_validation_clean[\"summary\"], maxlen=maxlen_summary, dtype='int32', padding='post', truncating='post',value=0)\n",
        "y_val = pad_sequences(df_validation_clean[\"title\"], maxlen=maxlen_title, dtype='int32', padding='post', truncating='post',value=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpxnDnYdGDcb"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9vgWcEHGFWy",
        "outputId": "646ba79c-eabc-452a-b698-b4a3688641e3"
      },
      "source": [
        "%tensorflow_version 1.15\n",
        "import tensorflow as tf\n",
        "import re           \n",
        "import os\n",
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords   \n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "import nltk\n",
        "import os\n",
        "from tensorflow.python.keras.layers import Layer\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.15`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oAsPMmQI54w"
      },
      "source": [
        "##Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msD-D28VI3_N"
      },
      "source": [
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "    There are three sets of weights introduced W_a, U_a, and V_a\n",
        "     \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        "\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\" Step function for computing energy for a single decoder state\n",
        "            inputs: (batchsize * 1 * de_in_dim)\n",
        "            states: (batchsize * 1 * de_latent_dim)\n",
        "            \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch size * en_seq_len * latent_dim\n",
        "            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "            if verbose:\n",
        "                print('Ua.h>', U_a_dot_h.shape)\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', Ws_plus_Uh.shape)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        "\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
        "        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\" Outputs produced by the layer \"\"\"\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5CFjYuwGHfW"
      },
      "source": [
        "## Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOwj8Y_xGE7B"
      },
      "source": [
        "from pythainlp import word_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6samYYQwwqL"
      },
      "source": [
        "word_vector_model = word_vector.get_model()\n",
        "embedding_weights = np.zeros([len(word2idx) + 1, 300])\n",
        "for word, i in word2idx.items():\n",
        "  try:\n",
        "    embedding_weights[i] = word_vector_model[word]\n",
        "  except:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bdu98YyWiJEM"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3wXbQzr8wr0e",
        "outputId": "a969fec8-8e62-4250-9598-e6878b43d065"
      },
      "source": [
        "MODEL_NAME = 'model_best_weights_news.h5'\n",
        "if load_weight :\n",
        "  !gdown --id 15fv8gMmnSQUCifKKZ7RNgDeTRxwEGD_p"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=15fv8gMmnSQUCifKKZ7RNgDeTRxwEGD_p\n",
            "To: /content/model_best_weights_news.h5\n",
            "424MB [00:02, 205MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "p6k28DsqiICi",
        "outputId": "5c5ee278-2b99-4190-b2d9-281d10938dc0"
      },
      "source": [
        "K.clear_session() \n",
        "latent_dim = 500\n",
        "x_voc_size = y_voc_size = len(word2idx) + 1\n",
        "\n",
        "# Encoder \n",
        "encoder_inputs = Input(shape=(maxlen_summary,)) \n",
        "enc_emb = Embedding(x_voc_size,300,weights=[embedding_weights],input_length=maxlen_summary,trainable=True)(encoder_inputs) \n",
        "\n",
        "#LSTM 1 \n",
        "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4) \n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb) \n",
        "\n",
        "#LSTM 2 \n",
        "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4) \n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1) \n",
        "\n",
        "#LSTM 3 \n",
        "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4) \n",
        "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2) \n",
        "\n",
        "# Set up the decoder. \n",
        "decoder_inputs = Input(shape=(None,)) \n",
        "dec_emb_layer = Embedding(x_voc_size,300,weights=[embedding_weights],input_length=maxlen_summary,trainable=True,) \n",
        "\n",
        "dec_emb = dec_emb_layer(decoder_inputs) \n",
        "\n",
        "#LSTM using encoder_states as initial state\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4) \n",
        "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c]) \n",
        "\n",
        "#Attention Layer\n",
        "attn_layer = AttentionLayer(name='attention_layer') \n",
        "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs]) \n",
        "\n",
        "# Concat attention output and decoder LSTM output \n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "#Dense layer\n",
        "decoder_dense = TimeDistributed(Dense(y_voc_size, activation='softmax')) \n",
        "decoder_outputs = decoder_dense(decoder_concat_input) \n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs) \n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 439)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 439, 300)     8483100     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 439, 500), ( 1602000     embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 439, 500), ( 2002000     lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 300)    8483100     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, 439, 500), ( 2002000     lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, None, 500),  1602000     embedding_1[0][0]                \n",
            "                                                                 lstm_2[0][1]                     \n",
            "                                                                 lstm_2[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "attention_layer (AttentionLayer ((None, None, 500),  500500      lstm_2[0][0]                     \n",
            "                                                                 lstm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concat_layer (Concatenate)      (None, None, 1000)   0           lstm_3[0][0]                     \n",
            "                                                                 attention_layer[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, None, 28277)  28305277    concat_layer[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 52,979,977\n",
            "Trainable params: 52,979,977\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zJssAvmXmf-B"
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nryvVTV8wtyT"
      },
      "source": [
        "if load_weight:\n",
        "  model.load_weights(MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4K94l-D8wwwZ"
      },
      "source": [
        "if plot_image:\n",
        "  tf.keras.utils.plot_model(\n",
        "    model, to_file='model.png', show_shapes=False, show_layer_names=True,\n",
        "    rankdir='TB', expand_nested=False, dpi=96\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is1bWyNMiL_O"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "41bJ-KPpmc8H"
      },
      "source": [
        "if not load_weight:\n",
        "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3)\n",
        "  checkpoint = ModelCheckpoint(MODEL_NAME, monitor='val_loss', verbose=1, save_best_only=True, mode='min', period=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RobCrTcxiN02"
      },
      "source": [
        "if not load_weight:\n",
        "  val_data = ([X_val,y_val[:, :-1]], y_val[:, 1:])\n",
        "  history=model.fit([X_train, y_train[:, :-1]],\n",
        "                    y_train[:, 1:],\n",
        "                    epochs = 30,\n",
        "                    batch_size=64,\n",
        "                    validation_data = val_data,\n",
        "                    callbacks = [es,checkpoint])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0QYGffEGxJmj"
      },
      "source": [
        "if not load_weight:\n",
        "  #download\n",
        "  from google.colab import files\n",
        "  files.download(MODEL_NAME) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DXQXCX4fiP9Y"
      },
      "source": [
        "if not load_weight:\n",
        "  from matplotlib import pyplot\n",
        "  pyplot.plot(history.history['loss'], label='train')\n",
        "  pyplot.plot(history.history['val_loss'], label='test') #might have error\n",
        "  pyplot.legend()\n",
        "  pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70Pxb4HJGUbZ"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YdziRGynGTuX"
      },
      "source": [
        "# encoder inference\n",
        "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
        "\n",
        "# decoder inference\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_hidden_state_input = Input(shape=(maxlen_summary,latent_dim))\n",
        "\n",
        "# Get the embeddings of the decoder sequence\n",
        "dec_emb2= dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "#attention inference\n",
        "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
        "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
        "\n",
        "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model(\n",
        "[decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
        "[decoder_outputs2] + [state_h2, state_c2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FiYkY9VxxSzT"
      },
      "source": [
        "if plot_image:\n",
        "  tf.keras.utils.plot_model(\n",
        "    encoder_model, to_file='encoder_model.png', show_shapes=False, show_layer_names=True,\n",
        "    rankdir='TB', expand_nested=False, dpi=96\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_2VEB8pcxXlZ"
      },
      "source": [
        "if plot_image:\n",
        "  tf.keras.utils.plot_model(\n",
        "    decoder_model, to_file='decoder_model.png', show_shapes=False, show_layer_names=True,\n",
        "    rankdir='TB', expand_nested=False, dpi=96\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "y1ifw0O_gFGE",
        "outputId": "3f621133-5dae-48f9-82cf-7af7e5f3f831"
      },
      "source": [
        "encoder_model.save('encoder')\n",
        "decoder_model.save('decoder')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 15). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 15). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: encoder/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: encoder/assets\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: decoder/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: decoder/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sZxvlGNaJedg",
        "outputId": "5a46268d-d8ba-4f69-f5b9-9bce37fb14c6"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Test loading model\n",
        "test_model = load_model('encoder')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiGpOqbFKBVc",
        "outputId": "b8e9c27a-bb81-447c-be6c-89ed3ddd3e8c"
      },
      "source": [
        "!zip -r encoder.zip encoder/\n",
        "!zip -r decoder.zip decoder/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: encoder/ (stored 0%)\n",
            "  adding: encoder/saved_model.pb (deflated 91%)\n",
            "  adding: encoder/assets/ (stored 0%)\n",
            "  adding: encoder/variables/ (stored 0%)\n",
            "  adding: encoder/variables/variables.data-00000-of-00001 (deflated 8%)\n",
            "  adding: encoder/variables/variables.index (deflated 52%)\n",
            "  adding: decoder/ (stored 0%)\n",
            "  adding: decoder/saved_model.pb (deflated 89%)\n",
            "  adding: decoder/assets/ (stored 0%)\n",
            "  adding: decoder/variables/ (stored 0%)\n",
            "  adding: decoder/variables/variables.data-00000-of-00001 (deflated 12%)\n",
            "  adding: decoder/variables/variables.index (deflated 45%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rJ4l4yLQdrI"
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
        "    \n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    \n",
        "    # Populate the first word of target sequence with the start word.\n",
        "    target_seq[0, 0] = word2idx[START_TOKEN]\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "      \n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = idx2word[sampled_token_index]\n",
        "        \n",
        "        if(sampled_token!=END_TOKEN):\n",
        "            decoded_sentence += ' '+sampled_token\n",
        "\n",
        "        # Exit condition: either hit max length or find stop word.\n",
        "        if (sampled_token == END_TOKEN  or len(decoded_sentence.split()) >= (maxlen_title-1)):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update internal states\n",
        "        e_h, e_c = h, c\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y78jfp-OQflP"
      },
      "source": [
        "def seqIdx2text(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "      if(i > word2idx[END_TOKEN]):\n",
        "        newString=newString + idx2word[i] + ' '\n",
        "    return newString"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFmSk8OXh07Y",
        "outputId": "cd4620ab-af4a-45cc-bf53-9deb6e8e7af4"
      },
      "source": [
        "for i in range(100, 110):\n",
        "  test_x = X_test[i:i+1]\n",
        "  test_y = y_test[i:i+1]\n",
        "  result = model.predict([test_x, test_y])\n",
        "  print(\"Actual headline:\", seqIdx2text(test_y[0]))\n",
        "  print(\"Actual content:\", seqIdx2text(test_x[0]))\n",
        "  print(\"Predicted headline (Inference):\",decode_sequence(test_x.reshape(1, maxlen_summary)))\n",
        "  print(\"Predicted headline (Train):\", seqIdx2text(np.argmax(result[0], axis=1)))\n",
        "  print(\"--------------------------------------------------------------------------\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual headline: ไทย คว้าแชมป์ ตะกร้อ ซูเปอร์ ซีรี่ส์ ที่ สิงคโปร์ \n",
            "Actual content: ทีม ชาติ ไทย คว้าแชมป์ การแข่งขัน ตะกร้อ  รายการ อิส  ซูเปอร์ ซีรี่ส์  ประเภท ชาย  และ หญิง  ที่ ประเทศ สิงคโปร์ \n",
            "Predicted headline (Inference):  ช้าง ศึก  -  1 9  ซ้อม  ลุย ศึก  ชิง แชมป์โลก  2  -  2  เซต  ชิง แชมป์โลก  2  ทีม\n",
            "Predicted headline (Train): ช้าง       \n",
            "--------------------------------------------------------------------------\n",
            "Actual headline: บัว  นลิน ทิพย์   -    ธรรม ์ ธัช  ซุ่ม ปลูก ต้น รัก ใน กอง ละคร  สุด หวาน \n",
            "Actual content: เอี๊ยด  เอี๊ยด ด  เป็น การบ่ม ความรัก แบบ เงียบๆ  ระหว่าง  บัว  -  นลิน ทิพย์  กับ พระเอก น้องใหม่   -  ธรรม ์ ธัช  ที่ เจอะ เจอกัน ใน กอง ละคร  คุณแม่ สวมรอย  งาน นี้ เลย แอบ ปลูก ต้น รัก กัน ไปมา  ไม่ต้อง สวมรอย ใครๆ \n",
            "Predicted headline (Inference):  ปอ  -  เขิน  -  ท้อง  -  ท้อง  -  ท้อง  -  รัก  -  รัก  -  รัก  -  รัก  -  รัก  -  รัก  -  รัก  -  รัก  -  รัก  -  รัก  -  รัก  -  รัก  -  รัก  -  รัก  -  รัก  -  รัก  -  รัก  -  รัก  -  รัก\n",
            "Predicted headline (Train): ปอ ขาว -   เผย (   รัก  รัก นัส   เขิน  ละคร หัวใจ  ละคร  ( รัก \n",
            "--------------------------------------------------------------------------\n",
            "Actual headline: สรุป ยังไง  ?   ไฮโซ เอย  บอก ดูใจ กัน  สวนทาง  ป้อง  ยืนกราน แค่ น้อง   (  คลิป  )  \n",
            "Actual content: เอย  ธัญ วรรณ  ยอมรับ คุย ป้อง  ณ วัฒน ์  เรียก ว่า ศึกษา ดูใจ ก็ได้  เผย รู้จัก กัน เพราะ ฝ่าย ชาย สร้างบ้าน และ ใช้ สถาปนิก คน เดียวกัน กับ บ้าน ของ ตน  เผย ที่ ฝ่าย ชาย ไม่ อยาก ให้ มี ข่าว เยอะ เพราะ กลัว คน พูดถึง ตน ไม่ ดี  รับ มี ไป ไหน กับ ป้อง บ้าง แต่ มี เพื่อน ไป ด้วย \n",
            "Predicted headline (Inference):  ปู  โพสต์ ภาพ  ทําไม ไม่ เคย เคย เคย ทํา ให้ เป็น คน ที่ ใช่  แต่ ไม่ อยาก ให้ ใคร ว่า ใคร ว่า จะ เป็น ใคร\n",
            "Predicted headline (Train): ปู ข่าว  1   หนุ่ม   ? ไม่  ไม่ ?  ?  ?  ไหน  ? (  คลิป  )  \n",
            "--------------------------------------------------------------------------\n",
            "Actual headline: รายงาน :  ศาลทหาร เตรียม ชี้ ชะตา คดี  1 1 2  เขียน น้ํา  คดี ที่  2  ของ ชาย สูงวัย \n",
            "Actual content: พรุ่งนี้   (  1 6  ต.ค.  )   ศาลทหาร มีนัด พิพากษา  คดี  1 1 2  ที่ นาย โอภาส  วัย  6 8  ปี ตกเป็น จํา เลย  คดี นี้ นับ เป็นคดี ที่สอง ของ เขา  เกิดขึ้น ใน ระหว่าง ที่ เขา ยัง ถูก คุมขัง ใน เรือน จํา  อัน เป็นผล จาก คดี แรก  ใน คดี แรก  โอภาส \n",
            "Predicted headline (Inference):  ศาลฎีกา  2  จํา คุก  2  ปี  2 5  ปี  2 5 6 2  กับ  2 5  ราย  คดี ฆ่า มนุษย์  -  ฟอกเงิน\n",
            "Predicted headline (Train): ศาลฎีกา :  คดี คุก คุก   ฆ่า 1 1 2  คดี   จํา - ฆ่า  1 จํา  \n",
            "--------------------------------------------------------------------------\n",
            "Actual headline: กรม อุตุฯ เตือน  1  -  2  วันนี้  กรุงเทพฯ และ ปริมณฑล  ฝุ่นละออง สะสม จํานวน มาก \n",
            "Actual content: กรมอุตุนิยมวิทยา  เผย สภาพอากาศ  ไทย ตอน บน มี หมอก ใน ตอนเช้า  ตอนกลางวัน มี แสง  เตือน ภาค กลาง  กรุงเทพฯ และ ปริมณฑล  ใน ระยะ  1  -  2  วันนี้ มี ฝุ่นละออง สะสม เป็น จํานวน มาก  เนื่องจาก ลม ที่ พัด ปกคลุม \n",
            "Predicted headline (Inference):  เตือน ภาคใต้ ระวัง ฝน ตกหนัก  -  อีสาน  -  ตะวันออก  -  ตะวันออก  -  ตะวันออก  -  ตะวันออก  -  ตะวันออก  -  ตะวันออก  -  ตะวันออก  -  ตะวันออก  -  ตะวันออก  -  ตะวันออก  -  ตะวันออก  -  ตะวันออก  -  ตะวันออก  -  ตะวันออก  -  ตะวันออก  -  ตะวันออก  -  ตะวันออก  -  ตะวันออก  -  ตะวันออก  -  ตะวันออก\n",
            "Predicted headline (Train): เตือน อุตุฯ   1  จังหวัด  1  มี  เหนือ  ฝน  - บางแห่ง  \n",
            "--------------------------------------------------------------------------\n",
            "Actual headline: อัษฎาวุธ  จูงมือ  น้อง สิงห์  สะบัด ปลาย  อยู่ บ้าน สาน สัมพันธ์  ครอบครัว  สู้ โควิด  -  1 9 \n",
            "Actual content: กลายเป็น  สงกรานต์  ที่ ต้อง จด จํา ไป อีก นาน  เป็น ปี ที่ ทุกๆ  คน อยู่ ใน สถานการณ์  อยู่ บ้าน  หยุด เชื้อ  เพื่อ ชาติ  กิจกรรม ต่างๆ  ทํา บุญ  ตักบาตร  สรง น้ํา พระ  รด น้ําดํา หัว \n",
            "Predicted headline (Inference):  รวม  5  ปี  ร่วม  5  ปี  ร่วม  5  หมื่น  ร่วม ร่วม  พระ เทพ ฯ  ทรง จํา ปี  6 3  ปี  6 0\n",
            "Predicted headline (Train): รวม  1  2 ปีใหม่  -    กับ กับ      ( โควิด  -  1 9 \n",
            "--------------------------------------------------------------------------\n",
            "Actual headline: จุด สรุป ทิศ ทางการเมือง ไทย \n",
            "Actual content: ปรากฏการณ์ ที่ เห็น จาก การ ออกตัว ทางการเมือง จาก การ ไป จด แจ้ง ชื่อ พรรคการเมือง นั้น  แม้ ส่วนใหญ่ ที่ ออกมา น้ํา หนัก จะ ไป ทาง  หนุน ลุง ตู่  มากกว่า  แต่ ที่ มองข้าม ไม่ ได้ แม้ จะ ยัง ไม่ ได้ ยื่น จด แจ้ง ชื่อ พรรค \n",
            "Predicted headline (Inference):  เลือกตั้ง  2 5 6 2 :  :  การเมือง ที่ ต้อง ไม่ มี ใคร อยู่ ใน  ? \n",
            "Predicted headline (Train): เลือกตั้ง การเมือง  การเมือง   \n",
            "--------------------------------------------------------------------------\n",
            "Actual headline: ชาวบ้าน ฟ้อง แพ่ง เหมือง ทอง พิจิตร  เรียก  5 0  ล้าน ฟื้นฟู สิ่งแวดล้อม \n",
            "Actual content: ชาวบ้าน พิจิตร เพชรบูรณ์  ฟ้อง แพ่ง  บ. อัค รา  รี ซอร์ส เซ ส  เรียกค่าเสียหาย จาก ผลกระทบ สุขภาพ และ แหล่ง น้ํา ธรรมชาติ  1 . 5 8  ล้าน บาท  และ ให้ จัดตั้ง กองทุนฟื้นฟู สิ่งแวดล้อม  5 0  ล้าน บาท  2 7  พ.ค. 2 5 5 9  ที่ ศาลแพ่ง  รัชดาภิเษก \n",
            "Predicted headline (Inference):  ชาวบ้าน ร้อง  จ. กระบี่  เตรียม ยื่น  3  โครงการ  จ. กระบี่  กว่า  2 0 0 0  ล้าน\n",
            "Predicted headline (Train): ชาวบ้าน ร้อง   ทอง   จ.  6   ล้าน   \n",
            "--------------------------------------------------------------------------\n",
            "Actual headline: วรงค์  มา ตาม นัด  โพสต์ เฟซฯ  1 5  ชื่อ  ท้า  คณะ ก้าวหน้า  โชว์ หลักฐาน โอน เงิน \n",
            "Actual content: หมอ วรงค์  มา ตาม นัด  โพสต์ เฟซฯ  1 5  รายชื่อ  ท้า ให้  คณะ ก้าวหน้า  โชว์ หลักฐาน การ โอน เงิน  โครงการ  เมย์ เดย์  เมย์ เดย์  หลัง สงสัย  ไม่ มี การ โอน เงิน จริง  ให้ เวลา  4 8  ชม. พิสูจน์ ความบริสุทธิ์ \n",
            "Predicted headline (Inference):  หมอ หมอ  ยัน  ไม่ เคย เคย เคย รับ  แต่ ถูก  แต่ ไม่ ได้  แต่ ไม่ มี ใคร   (  คลิป  ) \n",
            "Predicted headline (Train): หมอ  หมอ แล้ว   2 คลิป  - 0  ล้าน  ผอ.  หมอ   รับ ผลงาน  เงิน รางวัล \n",
            "--------------------------------------------------------------------------\n",
            "Actual headline: นับคะแนน เว็บ ประชามติ  ผู้ใช้  9 3 %  อยาก ให้ ทํา ประชามติ ร่างรัฐธรรมนูญ \n",
            "Actual content: เว็บไซต์  . org  เปิด ผล การ ออกเสียง ใน ประเด็น ร่างรัฐธรรมนูญ  สาม ประเด็น  เผย ผู้ใช้  9 3 %  อยาก ให้ มี กา รท ํา ประชามติ  8 7 %  ไม่ เห็นด้วย กับ การเขียน ที่มา  ส.ว.  และ  9 0 %  ไม่ เอา นายก คนนอก  1 8  พ.ค.  2 5 5 8  เวลา  1 2 . 3 0 \n",
            "Predicted headline (Inference):  เปิด ร่าง  รธน.  -  กม ธ.  ร่าง รธน. ใหม่  รธน.  6  คน\n",
            "Predicted headline (Train): เปิด    เปิด อํา 1    มาตรา ให้  ประชามติ \n",
            "--------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOXIO3yZxCwL"
      },
      "source": [
        "#Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bs9w-SCVlxaj",
        "outputId": "eae2a848-cfa1-4cc0-bb92-cb47811dc0ae"
      },
      "source": [
        "!pip install rouge"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUDRLZg80hMW"
      },
      "source": [
        "from rouge import Rouge\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykHN1cDQ0jOR"
      },
      "source": [
        "def evaluate_rouge_score(hypothesis, reference): \n",
        "  rouge = Rouge()\n",
        "  avg_scores = rouge.get_scores(hypothesis, reference, avg=True)\n",
        "  return avg_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGgn_118EijY"
      },
      "source": [
        "def evaluate_bleu_score(hypothesis, reference):\n",
        "  hypothesis = [e.split() for e in hypothesis]\n",
        "  reference = [[e.split()] for e in reference]\n",
        "  avg_scores = {}\n",
        "  smoothie = SmoothingFunction().method4\n",
        "  avg_scores[\"bleu-1\"] = corpus_bleu(reference, hypothesis, weights=(1,0,0,0), smoothing_function=smoothie)\n",
        "  avg_scores[\"bleu-2\"] = corpus_bleu(reference, hypothesis, weights=(0,1,0,0), smoothing_function=smoothie)\n",
        "  avg_scores[\"cumulative\"] = corpus_bleu(reference, hypothesis, smoothing_function=smoothie)\n",
        "  return avg_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUUVdwEFWdMf"
      },
      "source": [
        "def evaluate(X, y):\n",
        "  predictions = []\n",
        "  references = []\n",
        "  for i in tqdm(range(X.shape[0]), position=0):\n",
        "    predictions.append(decode_sequence(X[i].reshape(1, maxlen_summary)))\n",
        "    references.append(seqIdx2text(y[i]))\n",
        "  return evaluate_rouge_score(predictions, references), evaluate_bleu_score(predictions, references)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Db1fsbfXWghP",
        "outputId": "6f9396f3-27c9-4e11-e568-79ee1925fc8b"
      },
      "source": [
        "rouge_scores, bleu_scores = evaluate(X_test[:100], y_test[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [02:44<00:00,  1.65s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ge8gQriB7hm9",
        "outputId": "b525a27f-d42e-4a69-d533-b6681ce19bb0"
      },
      "source": [
        "print(rouge_scores)\n",
        "print(bleu_scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'rouge-1': {'f': 0.07293421168901013, 'p': 0.07144304382161684, 'r': 0.08653017398837831}, 'rouge-2': {'f': 0.01590428308985544, 'p': 0.01661266079765563, 'r': 0.01777729608936703}, 'rouge-l': {'f': 0.08717951230394273, 'p': 0.1077745613949174, 'r': 0.07939863722448862}}\n",
            "{'bleu-1': 0.050191407911527, 'bleu-2': 0.010217681030653042, 'cumulative': 0.006351494422156746}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoKYobltP0Xr",
        "outputId": "a36c6fee-ed09-443d-dbd6-001bdbcfc76a"
      },
      "source": [
        "# Download words\n",
        "!gdown --id 1NNUDoxFKEM43MAyPx2Fz7coDVoJuo2RI"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1NNUDoxFKEM43MAyPx2Fz7coDVoJuo2RI\n",
            "To: /content/word.txt\n",
            "\r  0% 0.00/548k [00:00<?, ?B/s]\r100% 548k/548k [00:00<00:00, 8.60MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rLAAIgjP4ZN"
      },
      "source": [
        "\n",
        "def loadDict():\n",
        "  word2idx = {}\n",
        "  with open(\"word.txt\", \"r\") as f:\n",
        "    for line in f.readlines():\n",
        "      word2idx[line.replace(\"\\n\",\"\")] = len(word2idx) + 1\n",
        "\n",
        "  idx2word = {v: k for k, v in word2idx.items()}\n",
        "  return word2idx, idx2word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOcuIMEfXMRo"
      },
      "source": [
        "#Generate title"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZtamhcCJZo2"
      },
      "source": [
        "# Format input function\n",
        "# Param sentence :string\n",
        "# Return tokenized sentence :list<string>\n",
        "def formatInput(sentence):  \n",
        "    from unicodedata import normalize\n",
        "    from pythainlp.tokenize import word_tokenize\n",
        "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "    START_TOKEN = \"<s>\"\n",
        "    END_TOKEN = \"</s>\"\n",
        "    UNK_TOKEN = \"UNK\"\n",
        "    MAXLEN_INPUT = 439\n",
        "\n",
        "    def cleanInput(sen):\n",
        "        sen = normalize(\"NFKD\", sen.strip().lower())\n",
        "        sen = \" \".join(sen.split())\n",
        "        return sen\n",
        "\n",
        "    # split number\n",
        "    def isNum(word):\n",
        "        return word.replace(\",\", \"\").replace(\".\", \"\").isnumeric()\n",
        "\n",
        "    def clearAfterToken(sen):\n",
        "        newSentence = []\n",
        "        for word in sen:\n",
        "            word = word.strip()\n",
        "            if isNum(word):\n",
        "              word = \"~\".join(list(word))\n",
        "            \n",
        "            word = word.replace(\"(\", \"~(~\").replace(\")\", \"~)~\").replace(\"–\", \"-\"). replace(\"-\", \"~-~\").replace(\"?\", \"~?~\")\n",
        "            word = word.replace(\"“\",'~\"~').replace(\"”\",'~\"~')\n",
        "            word = word.replace(\"‘\",\"~'~\").replace(\"’\",\"~'~\")\n",
        "            word = word.strip().split('~')\n",
        "            newSentence += word\n",
        "        return newSentence\n",
        "\n",
        "    def preprocessForKeras(sen):\n",
        "      sen = [START_TOKEN] + sen + [END_TOKEN]\n",
        "      word2idx, _ = loadDict()\n",
        "      sen = [word2idx.get(word, 1) for word in sen]\n",
        "      sen = pad_sequences([sen], maxlen=MAXLEN_INPUT, dtype='int32', padding='post', truncating='post',value=0)\n",
        "      sen = sen.reshape(1, MAXLEN_INPUT)\n",
        "      return sen\n",
        "\n",
        "    return preprocessForKeras(clearAfterToken(word_tokenize(cleanInput(sentence), engine=\"newmm\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyGmvs_cVpsD",
        "outputId": "afbdc3f3-3527-4196-dc9d-554bfcc6cfaa"
      },
      "source": [
        "input = formatInput(\"ปัตตานีตื่นทั้งตลาด เหตุ จนท.ติดเชื้อโควิด-19 จากการสัมผัสผู้ป่วยไปสถานบันเทิงแล้วมาช่วยแม่ขายของ พ่อค้าแม่ค้ารู้ข่าวต่างผวาระดมกำลังทำความสะอาดใหญ่ ด้าน ผวจ.สั่งตั้งด่านคัดกรองคนเข้า-ออก\")\n",
        "prediction = decode_sequence(input)\n",
        "print(prediction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " พบ โควิด  -  1 9  ราย  พบ ผู้ป่วย โควิด  -  1 9  ราย  พบ ติดเชื้อ  9  ราย  พบ ติดเชื้อ  6  ราย\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}